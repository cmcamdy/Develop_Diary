
## TODO 
round_type记得check一下，默认值为1
- /home/cmcandy/code/PD/Paddle/paddle/fluid/operators/fake_quantize_op.cc
- /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/operator/ir/ops.yaml
- /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/operator/ir/ops_backward.yaml
- 



## 算子定义
paddle/fluid/operators/fake_quantize_op.h
```
在这个文件中，共有以下 9 个 kernel 定义(GPT)：
FakeQuantizeAbsMaxKernel
FakeQuantizeDequantizeAbsMaxKernel
FakeChannelWiseQuantizeAbsMaxKernel
FakeChannelWiseQuantizeDequantizeAbsMaxKernel
FakeQuantizeRangeAbsMaxKernel
FakeQuantizeMovingAverageAbsMaxKernel
FakeQuantizeDequantizeMovingAverageAbsMaxKernel
MovingAverageAbsMaxScaleKernel
StraightThroughEstimatorGradKernel
```

```
fake_quantize_abs_max
fake_channel_wise_quantize_abs_max
fake_quantize_range_abs_max
fake_quantize_moving_average_abs_max

# 这个应该是有的
quantize_linear? : /home/cmcandy/code/PD/Paddle/paddle/fluid/operators/quantize_linear_op.cc

moving_average_abs_max_scale
fake_quantize_dequantize_abs_max
fake_channel_wise_quantize_dequantize_abs_max
```

### FakeQuantizeAbsMaxKernel

- 注册代码
```cpp
REGISTER_OPERATOR(
    fake_quantize_abs_max,
    ops::FakeQuantOrWithDequantAbsMaxOp,
    ops::FakeQuantOrWithDequantAbsMaxOpMaker,
    paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
    paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>);
PD_REGISTER_STRUCT_KERNEL(fake_quantize_abs_max,
                          CPU,
                          ALL_LAYOUT,
                          ops::FakeQuantizeAbsMaxKernel,
                          float) {}

```

- fake_quantize_abs_max 是kernel_name
- ops::FakeQuantizeAbsMaxKernel 是meta_kernel_structure
- ops::FakeQuantOrWithDequantAbsMaxOp ,ops::FakeQuantOrWithDequantAbsMaxOpMaker是前向定义，前者有infershape，后者由参数定义
- 后面俩EmptyGradOpMaker其实等价于这个宏REGISTER_OP_WITHOUT_GRADIENT，说明这个算子没有梯度反传


#### 需要补充：
- paddle/fluid/pir/dialect/operator/ir/ops.yaml 中的映射看情况补充
- paddle/fluid/pir/dialect/operator/ir/ops.yaml 中的映射看情况补充
```yaml
- op: fake_quantize_abs_max
  args: (Tensor x, int bit_length = 8)
  output: Tensor(out), Tensor(out_scale)
  infer_meta:
    func: FakeQuantizeInferMeta
    param: [x]
  kernel:
    func: fake_quantize_abs_max
    data_type: x
```

- paddle/fluid/pir/dialect/operator/ir/ops_backward.yaml 中的映射看情况补充
- InferShape





### FakeChannelWiseQuantizeAbsMaxKernel


- 注册代码
```cpp
REGISTER_OPERATOR(
    fake_channel_wise_quantize_abs_max,
    ops::FakeChannelWiseQuantizeAbsMaxOp,
    ops::FakeChannelWiseQuantizeAbsMaxOpMaker,
    paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
    paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>);
PD_REGISTER_STRUCT_KERNEL(fake_channel_wise_quantize_abs_max,
                          CPU,
                          ALL_LAYOUT,
                          ops::FakeChannelWiseQuantizeAbsMaxKernel,
                          float) {}
```


## 测试
```
export FLAGS_PIR_OPTEST=true
export FLAGS_PIR_OPTEST_WHITE_LIST=true
export FLAGS_enable_pir_in_executor=true

time cmake .. -DCMAKE_CUDA_COMPILER=/usr/local/cuda-11.7/bin/nvcc -DPY_VERSION=3.8 -DWITH_GPU=ON -DON_INFER=ON -DWITH_NVCC_LAZY=ON -DWITH_TESTING=ON  -DCMAKE_EXE_LINKER_FLAGS="-Wl,--copy-dt-needed-entries" && make -j 8 && pip install -U /home/cmcandy/code/PD/Paddle/build/python/dist/paddlepaddle_gpu-0.0.0-cp38-cp38-linux_x86_64.whl
cd ..               
python test/legacy_test/test_fake_quantize_op.py 
```

## pre-commit
```
pre-commit run --files  \
        /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/operator/utils/utils.cc          \
        /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/operator/ir/ops.yaml              \
        /home/cmcandy/code/PD/Paddle/paddle/phi/infermeta/unary.h \
        /home/cmcandy/code/PD/Paddle/paddle/phi/infermeta/unary.cc  \
        /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/op_generator/ops_api_gen.py
```
## add & commit 

```
git add /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/operator/utils/utils.cc          \
        /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/operator/ir/ops.yaml              \
        /home/cmcandy/code/PD/Paddle/paddle/phi/infermeta/unary.h \
        /home/cmcandy/code/PD/Paddle/paddle/phi/infermeta/unary.cc  \
        /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/op_generator/ops_api_gen.py
git commit -m "[PIR] fix fake_quantize_abs_max & fake_channel_wise_quantize_abs_max & fake_quantize_range_abs_max & fake_quantize_moving_average_abs_max"
```


## bug 记录


### ValueError: (InvalidArgument) Input out not found when parsing op straight_through_estimator_grad
- 暂时没思路
- 难道是要在map里面映射一下？确实
```
======================================================================
ERROR: test_fake_quantize_dequantize_moving_average_abs_max (__main__.TestFakeQuantizeMovingAverageAbsMaxOp)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "test/legacy_test/test_fake_quantize_op.py", line 358, in test_fake_quantize_dequantize_moving_average_abs_max
    self._fake_quantize_moving_average_abs_max(
  File "test/legacy_test/test_fake_quantize_op.py", line 340, in _fake_quantize_moving_average_abs_max
    self.check_grad(['X'], 'Out', user_defined_grads=gradient)
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 3001, in check_grad
    self.check_grad_with_place(
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 3303, in check_grad_with_place
    numeric_grads = self.check_grad_with_place_for_static(
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 3062, in check_grad_with_place_for_static
    analytic_grads = self._get_gradient(
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 3770, in _get_gradient
    self._check_ir_grad_output(
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 3599, in _check_ir_grad_output
    executor.run(
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 1789, in run
    res = self._run_impl(
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 1960, in _run_impl
    program, new_exe = self._executor_cache.get_program_and_executor(
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 936, in get_program_and_executor
    return self._get_cached_program_and_executor(
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 1083, in _get_program_and_executor
    "default": translate_to_pir(new_program.desc)
ValueError: (InvalidArgument) Input out not found when parsing op straight_through_estimator_grad
  [Hint: Expected legacy_input_vars.size() == 1UL, but received legacy_input_vars.size():0 != 1UL:1.] (at /home/cmcandy/code/PD/Paddle/paddle/fluid/ir_adaptor/translator/op_translator.cc:560)
```

- **解决**：
  - 这个问题需要在op_compat_gen.py特殊指定一下,具体原因待补充,简而言之就是字符串拼串出问题了(fwd与bwd的名称不同时)


### NotFoundError: (round_type) is not found in AttributeMap and RuntimeAttributeMap of (fake_quantize_abs_max) operator.

- 感觉上应该是少了某一环
- build/paddle/fluid/pir/dialect/operator/ir/pd_op.h
```
RuntimeError: (PreconditionNotMet) op [pd_op.fake_quantize_abs_max] kernel output args defs should equal op outputs
  [Hint: Expected op_item->num_results() == output_defs.size(), but received op_item->num_results():2 != output_defs.size():0.] (at /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/transforms/pd_op_to_kernel_pass.cc:2074)
```

- 这个是OPMaker的attr定义漏了。。。。
```
NotFoundError: (round_type) is not found in AttributeMap and RuntimeAttributeMap of (fake_quantize_abs_max) operator.
    [Hint: Expected iter != op_.RuntimeAttrs().end(), but received iter == op_.RuntimeAttrs().end().] (at /home/cmcandy/code/PD/Paddle/paddle/fluid/framework/operator.h:466)
    [operator < pd_kernel.legacy_kernel > error]
```

### FatalError: `Process abort signal` is detected by the operating system.
- 这个时我在InferMeta的时候out_scales写错了
```
test/legacy_test/test_fake_quantize_op.py中的class TestFakeQuantizeRangeAbsMaxOp(OpTest):测试，会出现下面的问题
terminate called after throwing an instance of 'common::enforce::EnforceNotMet'
  what():  (External) CUDA error(700), an illegal memory access was encountered. 
  [Hint: 'cudaErrorIllegalAddress'. The device encountered a load or store instruction on an invalid memory address. This leaves the process in an inconsistentstate and any further CUDA work will return the same error. To continue using CUDA, the process must be terminated and relaunched. ] (at /home/cmcandy/code/PD/Paddle/paddle/fluid/platform/device/gpu/gpu_info.cc:328)

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::ThreadPoolTempl<paddle::framework::StlThreadEnvironment>::WorkerLoop(int)
1   paddle::framework::PirInterpreter::RunInstructionBaseAsync(unsigned long)
2   paddle::framework::PirInterpreter::RunInstructionBase(paddle::framework::InstructionBase*)
3   paddle::framework::LegacyKernelInstruction::Run()
4   paddle::framework::StructKernelImpl<paddle::operators::FakeQuantizeRangeAbsMaxKernel<phi::dtype::float16, phi::GPUContext>, void>::Compute(phi::KernelContext*)
5   paddle::operators::FakeQuantizeRangeAbsMaxKernel<phi::dtype::float16, phi::GPUContext>::Compute(paddle::framework::ExecutionContext const&) const
6   phi::DenseTensor::~DenseTensor()
7   std::_Sp_counted_deleter<phi::Allocation*, std::function<void (phi::Allocation*)>, std::allocator<void>, (__gnu_cxx::_Lock_policy)2>::_M_dispose()
8   paddle::memory::allocation::CUDAAllocator::FreeImpl(phi::Allocation*)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1711027220 (unix time) try "date -d @1711027220" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0x3e8000049f4) received by PID 18932 (TID 0x7f96015fe700) from PID 18932 ***]

```


### RuntimeError: (PreconditionNotMet) op [pd_op.fake_quantize_dequantize_abs_max] kernel output args defs should equal op outputs

- 这个问题可能是LegacyOpList没加上
```
======================================================================
ERROR: test_fake_quantize_dequantize_abs_max (__main__.TestFakeQuantizeDequantizeAbsMaxOp)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "test/legacy_test/test_fake_quantize_op.py", line 401, in test_fake_quantize_dequantize_abs_max
    self._fake_quantize_dequantize_abs_max(
  File "test/legacy_test/test_fake_quantize_op.py", line 396, in _fake_quantize_dequantize_abs_max
    self.check_output(check_dygraph=False)
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 2793, in check_output
    res = self.check_output_with_place(
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 2643, in check_output_with_place
    static_checker.check()
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 2213, in check
    self.calculate_output()
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 2221, in calculate_output
    outs, fetch_list = self.op_test._calc_output(
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 1597, in _calc_output
    outs = executor.run(
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 1789, in run
    res = self._run_impl(
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 1960, in _run_impl
    program, new_exe = self._executor_cache.get_program_and_executor(
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 936, in get_program_and_executor
    return self._get_cached_program_and_executor(
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 1101, in _get_program_and_executor
    new_exe = _StandaloneExecutor(place, plan, scope)
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 819, in __init__
    self._new_exe = self._create_new_executor()
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 855, in _create_new_executor
    new_exe = core.StandaloneExecutor(self._place, self._plan, self._scope)
RuntimeError: (PreconditionNotMet) op [pd_op.fake_quantize_dequantize_abs_max] kernel output args defs should equal op outputs
  [Hint: Expected op_item->num_results() == output_defs.size(), but received op_item->num_results():2 != output_defs.size():0.] (at /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/transforms/pd_op_to_kernel_pass.cc:2074)

```

### PreconditionNotMetError: Tensor holds no memory. Call Tensor::mutable_data firstly.
- 测试文件: /home/cmcandy/code/PD/Paddle/test/legacy_test/test_fake_quantize_op.py

```cpp
TestFakeQuantizeRangeAbsMaxOp
....

def test_fake_quantize_range_abs_max(self):
        dtype_options = [np.float16, np.float32]
        is_test_options = [False, True]
        # is_test_options = [False]
        round_type_options = ['TiesToEven', 'TiesAwayFromZero']
        for dtype, is_test, round_type in itertools.product(
            dtype_options, is_test_options, round_type_options
        ):
            print("[dtype, is_test, round_type]:", dtype, is_test, round_type)
            self.attrs['bit_length'] = 8 if is_test else 5
            with self.subTest(
                dtype=dtype, is_test=is_test, round_type=round_type
            ):
                self._fake_quantize_range_abs_max(
                    dtype,
                    (8, 16, 6, 6),
                    lambda shape: (np.random.random(shape) - 0.4) * 10,
                    is_test=is_test,
                    round_type=round_type,
                )


```
- 错误分析:**当is_test设为true的时候,会报错**,完整log参考此文件:[test6.log](./test6.log)
- 主要分为下面两类:
```cpp
======================================================================
ERROR: test_fake_quantize_range_abs_max (__main__.TestFakeQuantizeRangeAbsMaxOp) (dtype=<class 'numpy.float16'>, is_test=True, round_type='TiesToEven')
----------------------------------------------------------------------
Traceback (most recent call last):
  File "test/legacy_test/test_fake_quantize_op.py", line 244, in test_fake_quantize_range_abs_max
    self._fake_quantize_range_abs_max(
  File "test/legacy_test/test_fake_quantize_op.py", line 229, in _fake_quantize_range_abs_max
    self.check_output(check_dygraph=False)
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 2793, in check_output
    res = self.check_output_with_place(
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 2643, in check_output_with_place
    static_checker.check()
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 2213, in check
    self.calculate_output()
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 2221, in calculate_output
    outs, fetch_list = self.op_test._calc_output(
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 1597, in _calc_output
    outs = executor.run(
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 1789, in run
    res = self._run_impl(
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 1995, in _run_impl
    ret = new_exe.run(
  File "/home/cmcandy/.local/lib/python3.8/site-packages/paddle/base/executor.py", line 833, in run
    tensors = self._new_exe.run(
RuntimeError: In user code:


    PreconditionNotMetError: Tensor holds no memory. Call Tensor::mutable_data firstly.
      [Hint: holder_ should not be null.] (at /home/cmcandy/code/PD/Paddle/paddle/phi/core/dense_tensor_impl.cc:41)
      [operator < pd_kernel.phi_kernel > error]


======================================================================
FAIL: test_fake_quantize_range_abs_max (__main__.TestFakeQuantizeRangeAbsMaxOp) (dtype=<class 'numpy.float32'>, is_test=True, round_type='TiesToEven')
----------------------------------------------------------------------
Traceback (most recent call last):
  File "test/legacy_test/test_fake_quantize_op.py", line 244, in test_fake_quantize_range_abs_max
    self._fake_quantize_range_abs_max(
  File "test/legacy_test/test_fake_quantize_op.py", line 229, in _fake_quantize_range_abs_max
    self.check_output(check_dygraph=False)
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 2793, in check_output
    res = self.check_output_with_place(
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 2643, in check_output_with_place
    static_checker.check()
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 2213, in check
    self.calculate_output()
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 2221, in calculate_output
    outs, fetch_list = self.op_test._calc_output(
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 1604, in _calc_output
    self._check_ir_output(place, program, feed_map, fetch_list, outs)
  File "/home/cmcandy/code/PD/Paddle/test/legacy_test/op_test.py", line 1504, in _check_ir_output
    check_method(
  File "/home/cmcandy/.local/lib/python3.8/site-packages/numpy/testing/_private/utils.py", line 985, in assert_array_equal
    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
  File "/home/cmcandy/anaconda3/envs/torch-py8/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/cmcandy/.local/lib/python3.8/site-packages/numpy/testing/_private/utils.py", line 778, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Arrays are not equal
Operator Check (fake_quantize_range_abs_max) has diff at Place(cpu)
Expect   - shape: [1]
  - layout: NCHW
  - place: Place(cpu)
  - dtype: float32
  - data: [5.99835]
But Got  - shape: []
  - layout: NCHW
uninited
 in class TestFakeQuantizeRangeAbsMaxOp
(shapes (1,), (0,) mismatch)
 x: array([5.998346], dtype=float32)
 y: array([], dtype=float64)

```
- 看了一下compute的逻辑在:
    - /home/cmcandy/code/PD/Paddle/paddle/fluid/operators/fake_quantize_op.h

```cpp
template <typename T, typename DeviceContext>
class FakeQuantizeRangeAbsMaxKernel : public framework::OpKernel<T> {
 public:
  void Compute(const framework::ExecutionContext &context) const override {
    .....
    // testing
    if (is_test) {
        ClipAndFakeQuantFunctor<DeviceContext, T>()(
            dev_ctx, *in, *in_scale, bin_cnt, round_type, out);
        return;
    }
    .....
  }};
```
- 疑问:不知到有没有遇到过类似问题

- 看起来是pd_op.memcpy_d2h这里报了错,下面的log中指出有两处
  - 0x6deeed01712919408568122682_inner_var_7 -> 0x6deeed01712919408568122682_inner_var_11
  - 0x6deeed01712919408568122682_inner_var_8 -> 0x6deeed01712919408568122682_inner_var_13

```shell
begin: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kCpuSync runs on HostTasks_thread_1
Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6deeed01712919408568122682_inner_var_7:[dtype=;place=;dim=1;lod={};]}, outputs:{0x6deeed01712919408568122682_inner_var_11:[dtype=;place=;dim=;lod={};]}.
I0412 18:56:48.578701 23938 phi_kernel_instruction.cc:183] Begin run op pd_op.memcpy_d2h infer meta.
I0412 18:56:48.578737 23938 phi_kernel_instruction.cc:187] End run op pd_op.memcpy_d2h infer meta.
I0412 18:56:48.578768 23938 phi_kernel_instruction.cc:188] Begin run op pd_op.memcpy_d2h kernel.
I0412 18:56:48.578670 23940 stats.h:91] HostMemoryStatReserved0: Update current_value with 18432, after update, current value = 129048
I0412 18:56:48.578632 23939 pir_interpreter.cc:1773] 
begin: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kCpuSync runs on HostTasks_thread_2
Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6deeed01712919408568122682_inner_var_8:[dtype=;place=;dim=1;lod={};]}, outputs:{0x6deeed01712919408568122682_inner_var_13:[dtype=;place=;dim=;lod={};]}.
I0412 18:56:48.578864 23939 phi_kernel_instruction.cc:183] Begin run op pd_op.memcpy_d2h infer meta.
I0412 18:56:48.578825 23940 stats.h:104] HostMemoryStatReserved0: Update current_value with 18432, after update, peak_value = 129048 , current value = 129048
I0412 18:56:48.578877 23938 op_call_stack.cc:93] PreconditionNotMetError: Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at /home/cmcandy/code/PD/Paddle/paddle/phi/core/dense_tensor_impl.cc:41)
I0412 18:56:48.578902 23939 phi_kernel_instruction.cc:187] End run op pd_op.memcpy_d2h infer meta.
I0412 18:56:48.579012 23939 phi_kernel_instruction.cc:188] Begin run op pd_op.memcpy_d2h kernel.
W0412 18:56:48.578979 23938 pir_interpreter.cc:1839]  OP id:6 pd_op.memcpy_d2h raises an EnforceNotMet exception common::enforce::EnforceNotMet
I0412 18:56:48.579069 23939 op_call_stack.cc:93] PreconditionNotMetError: Tensor holds no memory. Call Tensor::mutable_data firstly.
  [Hint: holder_ should not be null.] (at /home/cmcandy/code/PD/Paddle/paddle/phi/core/dense_tensor_impl.cc:41)
W0412 18:56:48.579074 23939 pir_interpreter.cc:1839]  OP id:8 pd_op.memcpy_d2h raises an EnforceNotMet exception common::enforce::EnforceNotMet
I0412 18:56:48.578941 23940 tensor_utils.cc:103] src:0x724804c00, dst:0x7fcb2c01a000
I0412 18:56:48.579093 23939 exception_holder.h:122] Non-first exception is discarded, the error message isIn user code:


    PreconditionNotMetError: Tensor holds no memory. Call Tensor::mutable_data firstly.
      [Hint: holder_ should not be null.] (at /home/cmcandy/code/PD/Paddle/paddle/phi/core/dense_tensor_impl.cc:41)
      [operator < pd_kernel.phi_kernel > error]
```

- 找出IR图

```
======================== The network executed by pir interpreter ========================
{
    (%0) = "(phi_kernel)" () {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"",name:"InScale",op_name:"pd_op.feed",persistable:[false],stop_gradient:[false]} : () -> cpu_tensor<1xf32>
    (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (cpu_tensor<1xf32>) -> gpu_tensor<1xf32>
    (%2) = "(phi_kernel)" () {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"",name:"Iter",op_name:"pd_op.feed",persistable:[false],stop_gradient:[false]} : () -> cpu_tensor<1xi64>
    (%3) = "shadow_feed(phi_kernel)" (%2) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (cpu_tensor<1xi64>) -> gpu_tensor<1xi64>
    (%4) = "(phi_kernel)" () {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"",name:"X",op_name:"pd_op.feed",persistable:[false],stop_gradient:[false]} : () -> cpu_tensor<8x16x6x6xf32>
    (%5) = "shadow_feed(phi_kernel)" (%4) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (cpu_tensor<8x16x6x6xf32>) -> gpu_tensor<8x16x6x6xf32>
    (%6, %7, %8) = "fake_quantize_range_abs_max(legacy_kernel)" (%5, %1, %3) {bit_length:(Int32)8,is_test:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"fake_quantize_range_abs_max",op_name:"pd_op.fake_quantize_range_abs_max",persistable:[false,false,false],round_type:(Int32)0,stop_gradient:[false,false,false],window_size:(Int32)1} : (gpu_tensor<8x16x6x6xf32>, gpu_tensor<1xf32>, gpu_tensor<1xi64>) -> gpu_tensor<8x16x6x6xf32>, gpu_tensor<1xf32>, gpu_tensor<1xf32>
    (%9) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h",persistable:[false,false,false]} : (gpu_tensor<8x16x6x6xf32>) -> cpu_tensor<8x16x6x6xf32>
    (%10) = "fetch(phi_kernel)" (%9) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"Out",op_name:"pd_op.fetch"} : (cpu_tensor<8x16x6x6xf32>) -> cpu_tensor<8x16x6x6xf32>
    (%11) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h",persistable:[false,false,false]} : (gpu_tensor<1xf32>) -> cpu_tensor<1xf32>
    (%12) = "fetch(phi_kernel)" (%11) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"OutScale",op_name:"pd_op.fetch"} : (cpu_tensor<1xf32>) -> cpu_tensor<1xf32>
    (%13) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h",persistable:[false,false,false]} : (gpu_tensor<1xf32>) -> cpu_tensor<1xf32>
    (%14) = "fetch(phi_kernel)" (%13) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"OutScales",op_name:"pd_op.fetch"} : (cpu_tensor<1xf32>) -> cpu_tensor<1xf32>
}

======================== The instruction executed by pir interpreter ========================
{outputs} =  instruction_name[idx] ({inputs})
0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1: ( 3 )  = pd_op.shadow_feed ( 2 ) 
2: ( 5 )  = pd_op.shadow_feed ( 4 ) 
3: ( 8 ) ( 7 ) ( 6 )  = pd_op.fake_quantize_range_abs_max ( 3 )  ( 1 )  ( 5 ) 
4: ( 9 )  = pd_op.memcpy_d2h ( 6 ) 
5: ( 10 )  = pd_op.fetch ( 9 ) 
6: ( 11 )  = pd_op.memcpy_d2h ( 7 ) 
7: ( 12 )  = pd_op.fetch ( 11 ) 
8: ( 13 )  = pd_op.memcpy_d2h ( 8 ) 
9: ( 14 )  = pd_op.fetch ( 13 ) 
---------------------------var_id -> var_name -> variable*---------------------------
0 -> InScale -> 0x7f6a970
1 -> 0x6deeed01712919408568122682_inner_var_1 -> 0x4458eac0
2 -> Iter -> 0x7f65030
3 -> 0x6deeed01712919408568122682_inner_var_3 -> 0x6df8bb0
4 -> X -> 0x7f3f1e0
5 -> 0x6deeed01712919408568122682_inner_var_5 -> 0x6e1f540
6 -> 0x6deeed01712919408568122682_inner_var_6 -> 0x6dbf6f0
7 -> 0x6deeed01712919408568122682_inner_var_7 -> 0x6e2c6a0
8 -> 0x6deeed01712919408568122682_inner_var_8 -> 0x6e07b20
9 -> 0x6deeed01712919408568122682_inner_var_9 -> 0x6df4940
10 -> Out@fetch -> 0x6e07620
11 -> 0x6deeed01712919408568122682_inner_var_11 -> 0x6e075e0
12 -> OutScale@fetch -> 0x6df9470
13 -> 0x6deeed01712919408568122682_inner_var_13 -> 0x6e07600
14 -> OutScales@fetch -> 0x6e33890


======================= The dependency of all instruction ========================
id -> down_stream_id
0 -> 3 
1 -> 3 
2 -> 3 
3 -> 4 6 8 
4 -> 5 
6 -> 7 
8 -> 9 

```

- 发现好像是memcpy_d2h导致的问题,具体是out_scale没取到导致的,或者说是取的时候发现内存没有初始化,没有执行这一句:`out_scale->mutable_data<T>(context.GetPlace());`
- 但是实际上,执行了也不合理, 因为会分配一个地址,但并不会往里面填值,所以这句执行了也会导致value check的时候报错
- **最终的解决办法是:**
  - 发现只要不check,它实际上不会执行memcpy_d2h,那么在is_test状态下执行的结果中没有out_scale也不会报错,具体而言当is_test状态时,加上`no_check_set = ['OutScale', 'OutScales'] if is_test else None`


