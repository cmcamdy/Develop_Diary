
## 算子定义
paddle/fluid/operators/fake_quantize_op.h
```
在这个文件中，共有以下 9 个 kernel 定义(GPT)：
FakeQuantizeAbsMaxKernel
FakeQuantizeDequantizeAbsMaxKernel
FakeChannelWiseQuantizeAbsMaxKernel
FakeChannelWiseQuantizeDequantizeAbsMaxKernel
FakeQuantizeRangeAbsMaxKernel
FakeQuantizeMovingAverageAbsMaxKernel
FakeQuantizeDequantizeMovingAverageAbsMaxKernel
MovingAverageAbsMaxScaleKernel
StraightThroughEstimatorGradKernel
```

```
fake_quantize_abs_max
fake_channel_wise_quantize_abs_max
fake_quantize_range_abs_max
fake_quantize_moving_average_abs_max

# 这个应该是有的
quantize_linear? : /home/cmcandy/code/PD/Paddle/paddle/fluid/operators/quantize_linear_op.cc

moving_average_abs_max_scale
fake_quantize_dequantize_abs_max
fake_channel_wise_quantize_dequantize_abs_max
```

### FakeQuantizeAbsMaxKernel

- 注册代码
```cpp
REGISTER_OPERATOR(
    fake_quantize_abs_max,
    ops::FakeQuantOrWithDequantAbsMaxOp,
    ops::FakeQuantOrWithDequantAbsMaxOpMaker,
    paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
    paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>);
PD_REGISTER_STRUCT_KERNEL(fake_quantize_abs_max,
                          CPU,
                          ALL_LAYOUT,
                          ops::FakeQuantizeAbsMaxKernel,
                          float) {}

```

- fake_quantize_abs_max 是kernel_name
- ops::FakeQuantizeAbsMaxKernel 是meta_kernel_structure
- ops::FakeQuantOrWithDequantAbsMaxOp ,ops::FakeQuantOrWithDequantAbsMaxOpMaker是前向定义，前者有infershape，后者由参数定义
- 后面俩EmptyGradOpMaker其实等价于这个宏REGISTER_OP_WITHOUT_GRADIENT，说明这个算子没有梯度反传


#### 需要补充：
- paddle/fluid/pir/dialect/operator/ir/ops.yaml 中的映射看情况补充
- paddle/fluid/pir/dialect/operator/ir/ops.yaml 中的映射看情况补充
```yaml
- op: fake_quantize_abs_max
  args: (Tensor x, int bit_length = 8)
  output: Tensor(out), Tensor(out_scale)
  infer_meta:
    func: FakeQuantizeInferMeta
    param: [x]
  kernel:
    func: fake_quantize_abs_max
    data_type: x
```

- paddle/fluid/pir/dialect/operator/ir/ops_backward.yaml 中的映射看情况补充
- InferShape





### FakeChannelWiseQuantizeAbsMaxKernel


- 注册代码
```cpp
REGISTER_OPERATOR(
    fake_channel_wise_quantize_abs_max,
    ops::FakeChannelWiseQuantizeAbsMaxOp,
    ops::FakeChannelWiseQuantizeAbsMaxOpMaker,
    paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
    paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>);
PD_REGISTER_STRUCT_KERNEL(fake_channel_wise_quantize_abs_max,
                          CPU,
                          ALL_LAYOUT,
                          ops::FakeChannelWiseQuantizeAbsMaxKernel,
                          float) {}
```


## 测试
```
export FLAGS_PIR_OPTEST=true
export FLAGS_PIR_OPTEST_WHITE_LIST=true
export FLAGS_enable_pir_in_executor=true

time cmake .. -DCMAKE_CUDA_COMPILER=/usr/local/cuda-11.7/bin/nvcc -DPY_VERSION=3.8 -DWITH_GPU=ON -DON_INFER=ON -DWITH_NVCC_LAZY=ON -DWITH_TESTING=ON  -DCMAKE_EXE_LINKER_FLAGS="-Wl,--copy-dt-needed-entries" && make -j 8
pip install -U /home/cmcandy/code/PD/Paddle/build/python/dist/paddlepaddle_gpu-0.0.0-cp38-cp38-linux_x86_64.whl
cd ..               
python test/legacy_test/test_partial_concat_op.py 
```

## pre-commit
```
pre-commit run --files  \
        /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/operator/utils/utils.cc          \
        /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/operator/ir/ops.yaml              \
        /home/cmcandy/code/PD/Paddle/paddle/phi/infermeta/unary.h \
        /home/cmcandy/code/PD/Paddle/paddle/phi/infermeta/unary.cc  \
        /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/op_generator/ops_api_gen.py
```
## add & commit 

```
git add /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/operator/utils/utils.cc          \
        /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/operator/ir/ops.yaml              \
        /home/cmcandy/code/PD/Paddle/paddle/phi/infermeta/unary.h \
        /home/cmcandy/code/PD/Paddle/paddle/phi/infermeta/unary.cc  \
        /home/cmcandy/code/PD/Paddle/paddle/fluid/pir/dialect/op_generator/ops_api_gen.py
git commit -m "[PIR] fix fake_quantize_abs_max & fake_channel_wise_quantize_abs_max & fake_quantize_range_abs_max & fake_quantize_moving_average_abs_max"
```