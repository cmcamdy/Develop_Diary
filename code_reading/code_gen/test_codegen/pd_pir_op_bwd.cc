// This file is generated by "paddle/fluid/pir/dialect/op_generator/op_gen.py"
#include "/home/cmcandy/code/PD/Develop_Diary/code_reading/code_gen/test_codegen/pd_op.h"
#include "paddle/fluid/pir/dialect/kernel/ir/kernel_type.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_meta_tensor.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_selected_rows.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_tensor.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_attribute.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_type.h"
#include "paddle/fluid/primitive/rule/vjp/vjp.h"
#include "paddle/phi/api/lib/utils/allocator.h"
#include "paddle/phi/core/dense_tensor.h"
#include "paddle/phi/core/enforce.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/unary.h"
#include "paddle/pir/include/core/builtin_attribute.h"
#include "paddle/pir/include/core/builtin_op.h"
#include "paddle/pir/include/core/builtin_type.h"
#include "paddle/pir/include/core/ir_context.h"
#include "paddle/pir/include/core/op_base.h"

using namespace paddle::dialect;

namespace paddle {
namespace dialect {

void PartialSumGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PartialSumGradInferMeta);
  fn(infer_meta);
}


std::vector<pir::Type> PartialSumGradOp::InferMeta(const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes) {
  PADDLE_ENFORCE_NOT_NULL(
        p_attributes, common::errors::Fatal("AttrtibueMap pointer in InferMeta function is nullptr."));
  auto& attributes = *p_attributes; (void)attributes;

  PADDLE_ENFORCE_EQ(input_values.size() == 2, true, phi::errors::InvalidArgument(
      "Num of inputs is expected to be 2 but got %d.", input_values.size()));

  pir::Value x_ = input_values[0]; (void)x_;
  VLOG(4) << "Builder construction outputs";
  bool is_from_tensor = false; (void) is_from_tensor;
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;


  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    if(x[i].isa<paddle::dialect::DenseTensorType>()) {
        auto x_type = x[i].dyn_cast<paddle::dialect::DenseTensorType>();
        vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_type.dtype()),
                                                                    x_type.dims(),
                                                                    x_type.data_layout(),
                                                                    x_type.lod(),
                                                                    x_type.offset()));
    } else {
        PADDLE_THROW(phi::errors::Unimplemented("Only support DenseTensorType or AllocatedDenseTensorType"));
    }
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_dense_x_grad((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_x_grad.push_back(paddle::dialect::IrMetaTensor(&vec_dense_x_grad[i]));
  }
  std::vector<phi::MetaTensor*> meta_x_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x_grad.size()); i++) {
    meta_x_grad.push_back(&vec_meta_x_grad[i]);
  }

  phi::PartialSumGradInferMeta(meta_x, meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> x_grad_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    x_grad_types.push_back(CvtToDenseTensorType(vec_dense_x_grad[i]));
  }
  pir::Type x_grad_type = pir::VectorType::get(pir::IrContext::Instance(), x_grad_types);

  argument_outputs.push_back(x_grad_type);

  return argument_outputs;
}

const char *PartialSumGradOp::attributes_name[2] = { "start_index", "length" };

OpInfoTuple PartialSumGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("start_index", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("length", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PartialSumGradInferMeta", {"x"}, "partial_sum_grad", {"x", "out_grad", "start_index", "length"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "partial_sum_grad");
}

void PartialSumGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int start_index, int length) {
  VLOG(4) << "Start build PartialSumGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_start_index = pir::Int32Attribute::get(pir::IrContext::Instance(), start_index);
  argument_attributes.insert({"start_index", attr_start_index});
  pir::Attribute attr_length = pir::Int32Attribute::get(pir::IrContext::Instance(), length);
  argument_attributes.insert({"length", attr_length});


  std::vector<pir::Type> argument_outputs = PartialSumGradOp::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void PartialSumGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PartialSumGradOp";


  PADDLE_ENFORCE_NE(
      attributes.find("start_index"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'start_index' Attribute is expected for PartialSumGradOp. "));
  int start_index = attributes.at("start_index").dyn_cast<pir::Int32Attribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("length"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'length' Attribute is expected for PartialSumGradOp. "));
  int length = attributes.at("length").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_start_index = pir::Int32Attribute::get(pir::IrContext::Instance(), start_index);
  argument_attributes.insert({"start_index", attr_start_index});
  pir::Attribute attr_length = pir::Int32Attribute::get(pir::IrContext::Instance(), length);
  argument_attributes.insert({"length", attr_length});


  std::vector<pir::Type> argument_outputs = PartialSumGradOp::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void PartialSumGradOp::VerifySig() {}

phi::DataType PartialSumGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PartialSumGradOp";
  


  return expected_kernel_dtype;
}

void StraightThroughEstimatorGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}


std::vector<pir::Type> StraightThroughEstimatorGradOp::InferMeta(const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes) {
  PADDLE_ENFORCE_NOT_NULL(
        p_attributes, common::errors::Fatal("AttrtibueMap pointer in InferMeta function is nullptr."));
  auto& attributes = *p_attributes; (void)attributes;

  PADDLE_ENFORCE_EQ(input_values.size() == 2, true, phi::errors::InvalidArgument(
      "Num of inputs is expected to be 2 but got %d.", input_values.size()));

  pir::Value x_ = input_values[0]; (void)x_;
  VLOG(4) << "Builder construction outputs";
  bool is_from_tensor = false; (void) is_from_tensor;

  paddle::dialect::DenseTensorType x;
  if (x_.type().isa<paddle::dialect::DenseTensorType>()) {
    x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support paddle::dialect::DenseTensorType or paddle::dialect::AllocatedDenseTensorType"));
  }



  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_type = CvtToDenseTensorType(dense_x_grad);

  argument_outputs.push_back(x_grad_type);

  return argument_outputs;
}

const char *StraightThroughEstimatorGradOp::attributes_name[4] = { "moving_rate", "bit_length", "round_type", "is_test" };

OpInfoTuple StraightThroughEstimatorGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("moving_rate", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("bit_length", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("round_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "straight_through_estimator_grad", {"x", "out_grad", "moving_rate", "bit_length", "round_type", "is_test"}, {"x_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "straight_through_estimator_grad");
}

void StraightThroughEstimatorGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int moving_rate, int bit_length, int round_type, bool is_test) {
  VLOG(4) << "Start build StraightThroughEstimatorGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_moving_rate = pir::Int32Attribute::get(pir::IrContext::Instance(), moving_rate);
  argument_attributes.insert({"moving_rate", attr_moving_rate});
  pir::Attribute attr_bit_length = pir::Int32Attribute::get(pir::IrContext::Instance(), bit_length);
  argument_attributes.insert({"bit_length", attr_bit_length});
  pir::Attribute attr_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), round_type);
  argument_attributes.insert({"round_type", attr_round_type});
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument_attributes.insert({"is_test", attr_is_test});


  std::vector<pir::Type> argument_outputs = StraightThroughEstimatorGradOp::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void StraightThroughEstimatorGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build StraightThroughEstimatorGradOp";


  PADDLE_ENFORCE_NE(
      attributes.find("moving_rate"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'moving_rate' Attribute is expected for StraightThroughEstimatorGradOp. "));
  int moving_rate = attributes.at("moving_rate").dyn_cast<pir::Int32Attribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("bit_length"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'bit_length' Attribute is expected for StraightThroughEstimatorGradOp. "));
  int bit_length = attributes.at("bit_length").dyn_cast<pir::Int32Attribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("round_type"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'round_type' Attribute is expected for StraightThroughEstimatorGradOp. "));
  int round_type = attributes.at("round_type").dyn_cast<pir::Int32Attribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("is_test"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'is_test' Attribute is expected for StraightThroughEstimatorGradOp. "));
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_moving_rate = pir::Int32Attribute::get(pir::IrContext::Instance(), moving_rate);
  argument_attributes.insert({"moving_rate", attr_moving_rate});
  pir::Attribute attr_bit_length = pir::Int32Attribute::get(pir::IrContext::Instance(), bit_length);
  argument_attributes.insert({"bit_length", attr_bit_length});
  pir::Attribute attr_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), round_type);
  argument_attributes.insert({"round_type", attr_round_type});
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument_attributes.insert({"is_test", attr_is_test});


  std::vector<pir::Type> argument_outputs = StraightThroughEstimatorGradOp::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void StraightThroughEstimatorGradOp::VerifySig() {}

phi::DataType StraightThroughEstimatorGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: StraightThroughEstimatorGradOp";
  


  return expected_kernel_dtype;
}

void StraightThroughEstimatorGrad2Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}


std::vector<pir::Type> StraightThroughEstimatorGrad2Op::InferMeta(const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes) {
  PADDLE_ENFORCE_NOT_NULL(
        p_attributes, common::errors::Fatal("AttrtibueMap pointer in InferMeta function is nullptr."));
  auto& attributes = *p_attributes; (void)attributes;

  PADDLE_ENFORCE_EQ(input_values.size() == 2, true, phi::errors::InvalidArgument(
      "Num of inputs is expected to be 2 but got %d.", input_values.size()));

  pir::Value x_ = input_values[0]; (void)x_;
  VLOG(4) << "Builder construction outputs";
  bool is_from_tensor = false; (void) is_from_tensor;

  paddle::dialect::DenseTensorType x;
  if (x_.type().isa<paddle::dialect::DenseTensorType>()) {
    x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support paddle::dialect::DenseTensorType or paddle::dialect::AllocatedDenseTensorType"));
  }



  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_x, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_type = CvtToDenseTensorType(dense_x_grad);

  argument_outputs.push_back(x_grad_type);

  return argument_outputs;
}

const char *StraightThroughEstimatorGrad2Op::attributes_name[3] = { "bit_length", "round_type", "is_test" };

OpInfoTuple StraightThroughEstimatorGrad2Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("bit_length", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("round_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "straight_through_estimator_grad", {"x", "out_grad", "bit_length", "round_type", "is_test"}, {"x_grad"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "straight_through_estimator_grad2");
}

void StraightThroughEstimatorGrad2Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int bit_length, int round_type, bool is_test) {
  VLOG(4) << "Start build StraightThroughEstimatorGrad2Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_bit_length = pir::Int32Attribute::get(pir::IrContext::Instance(), bit_length);
  argument_attributes.insert({"bit_length", attr_bit_length});
  pir::Attribute attr_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), round_type);
  argument_attributes.insert({"round_type", attr_round_type});
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument_attributes.insert({"is_test", attr_is_test});


  std::vector<pir::Type> argument_outputs = StraightThroughEstimatorGrad2Op::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void StraightThroughEstimatorGrad2Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build StraightThroughEstimatorGrad2Op";


  PADDLE_ENFORCE_NE(
      attributes.find("bit_length"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'bit_length' Attribute is expected for StraightThroughEstimatorGrad2Op. "));
  int bit_length = attributes.at("bit_length").dyn_cast<pir::Int32Attribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("round_type"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'round_type' Attribute is expected for StraightThroughEstimatorGrad2Op. "));
  int round_type = attributes.at("round_type").dyn_cast<pir::Int32Attribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("is_test"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'is_test' Attribute is expected for StraightThroughEstimatorGrad2Op. "));
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_bit_length = pir::Int32Attribute::get(pir::IrContext::Instance(), bit_length);
  argument_attributes.insert({"bit_length", attr_bit_length});
  pir::Attribute attr_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), round_type);
  argument_attributes.insert({"round_type", attr_round_type});
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument_attributes.insert({"is_test", attr_is_test});


  std::vector<pir::Type> argument_outputs = StraightThroughEstimatorGrad2Op::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void StraightThroughEstimatorGrad2Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: StraightThroughEstimatorGrad2Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  PADDLE_ENFORCE_EQ(input_size == 2u, true, phi::errors::InvalidArgument(
                    "The size %d of inputs must be equal to 2.", input_size));
  PADDLE_ENFORCE_EQ((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(), true,
                  phi::errors::InvalidArgument("Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type()));
  PADDLE_ENFORCE_EQ((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(), true,
                  phi::errors::InvalidArgument("Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type()));
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  PADDLE_ENFORCE_GT(attributes.count("bit_length"), 0, phi::errors::InvalidArgument(
                 "bit_length does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("bit_length").isa<pir::Int32Attribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: bit_length is not pir::Int32Attribute."));

  PADDLE_ENFORCE_GT(attributes.count("round_type"), 0, phi::errors::InvalidArgument(
                 "round_type does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("round_type").isa<pir::Int32Attribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: round_type is not pir::Int32Attribute."));

  PADDLE_ENFORCE_GT(attributes.count("is_test"), 0, phi::errors::InvalidArgument(
                 "is_test does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("is_test").isa<pir::BoolAttribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: is_test is not pir::BoolAttribute."));

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  PADDLE_ENFORCE_EQ(output_size == 1u, true, phi::errors::InvalidArgument(
                    "The size %d of outputs must be equal to 1.", output_size));
  PADDLE_ENFORCE_EQ((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(), true, phi::errors::InvalidArgument(
                 "Type validation failed for the 0th output."));
  }
  VLOG(4) << "End Verifying for: StraightThroughEstimatorGrad2Op.";
}

phi::DataType StraightThroughEstimatorGrad2Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: StraightThroughEstimatorGrad2Op";
  


  return expected_kernel_dtype;
}

} // namespace dialect
} // namespace paddle


IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PartialSumGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::StraightThroughEstimatorGradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::StraightThroughEstimatorGrad2Op)

