// This file is generated by "paddle/fluid/pir/dialect/op_generator/op_gen.py"
#pragma once
#include <vector>

#include "paddle/fluid/pir/dialect/operator/interface/decomp.h"
#include "paddle/fluid/pir/dialect/operator/interface/infer_symbolic_shape/infer_symbolic_shape.h"
#include "paddle/fluid/pir/dialect/operator/interface/infermeta.h"
#include "paddle/fluid/pir/dialect/operator/interface/op_yaml_info.h"
#include "paddle/fluid/pir/dialect/operator/interface/parse_kernel_key.h"
#include "paddle/fluid/pir/dialect/operator/interface/vjp.h"
#include "paddle/fluid/pir/dialect/operator/trait/inplace.h"
#include "paddle/fluid/pir/dialect/operator/utils/op_yaml_info_util.h"
#include "paddle/fluid/pir/dialect/operator/utils/utils.h"
#include "paddle/pir/include/core/builder.h"
#include "paddle/pir/include/core/op_base.h"
#include "paddle/pir/include/core/op_trait.h"
#include "paddle/pir/include/core/operation_utils.h"
#ifdef PADDLE_WITH_DNNL
#include "paddle/fluid/pir/dialect/operator/trait/onednn.h"
#endif
#include "paddle/fluid/framework/infershape_utils.h"
#include "paddle/fluid/ir_adaptor/translator/utils.h"
#include "paddle/fluid/pir/dialect/operator/ir/manual_op.h"
#include "paddle/fluid/pir/dialect/operator/trait/custom_vjp.h"
#include "paddle/phi/core/infermeta_utils.h"
#ifdef PADDLE_WITH_DISTRIBUTE
#include "paddle/phi/infermeta/spmd_rules/rules.h"
#include "paddle/fluid/pir/dialect/distributed/ir/dist_type.h"
#include "paddle/fluid/pir/dialect/distributed/ir/dist_tools.h"
#endif

#include "paddle/phi/common/data_type.h"
#include "paddle/fluid/pir/dialect/operator/interface/get_kernel_type_for_var.h"
            

namespace paddle {
namespace dialect {

extern std::unordered_map<std::string, std::vector<PdOpSig>> op_to_multi_kernels_map;

} // namespace dialect
} // namespace paddle

namespace paddle {
namespace dialect {

class  PartialSumOp : public pir::Op<PartialSumOp,paddle::dialect::InferMetaInterface,paddle::dialect::VjpInterface,paddle::dialect::OpYamlInfoInterface,paddle::dialect::GetKernelTypeForVarInterface> {
 public:
  using Op::Op;
  static const char *name() { return "pd_op.partial_sum"; }
  static const char *attributes_name[2];
  static constexpr uint32_t attributes_num = 2;
  static OpInfoTuple GetOpInfo();
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int start_index=0, int length=-1);
  
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes);
  
  void VerifySig();

  static phi::DataType GetKernelTypeForVar(
      const std::string& var_name,
        const phi::DataType& tensor_dtype,
        const phi::DataType& expected_kernel_dtype);




  static std::vector<std::vector<pir::Value>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);
  static void InferMeta(phi::InferMetaContext *infer_meta );
  static std::vector<pir::Type> InferMeta( const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes );
  pir::Value x() { return operand_source(0); }
  pir::Value out() { return result(0); }

};

class  FakeQuantizeDequantizeMovingAverageAbsMaxOp : public pir::Op<FakeQuantizeDequantizeMovingAverageAbsMaxOp,paddle::dialect::InferMetaInterface,paddle::dialect::VjpInterface,paddle::dialect::OpYamlInfoInterface,paddle::dialect::GetKernelTypeForVarInterface> {
 public:
  using Op::Op;
  static const char *name() { return "pd_op.fake_quantize_dequantize_moving_average_abs_max"; }
  static const char *attributes_name[4];
  static constexpr uint32_t attributes_num = 4;
  static OpInfoTuple GetOpInfo();
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value in_scale_, pir::Value in_accum_, pir::Value in_state_, float moving_rate=0.9, int bit_length=8, int round_type=0, bool is_test=false);
  
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value in_scale_, pir::Value in_accum_, pir::Value in_state_, pir::AttributeMap attributes);
  
  void VerifySig();

  static phi::DataType GetKernelTypeForVar(
      const std::string& var_name,
        const phi::DataType& tensor_dtype,
        const phi::DataType& expected_kernel_dtype);




  static std::vector<std::vector<pir::Value>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);
  static void InferMeta(phi::InferMetaContext *infer_meta );
  static std::vector<pir::Type> InferMeta( const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes );
  pir::Value x() { return operand_source(0); }
  pir::Value in_scale() { return operand_source(1); }
  pir::Value in_accum() { return operand_source(2); }
  pir::Value in_state() { return operand_source(3); }
  pir::Value out() { return result(0); }
  pir::Value out_scale() { return result(1); }
  pir::Value out_state() { return result(2); }
  pir::Value out_accum() { return result(3); }

};

class  MovingAverageAbsMaxScaleOp : public pir::Op<MovingAverageAbsMaxScaleOp,paddle::dialect::InferMetaInterface,paddle::dialect::VjpInterface,paddle::dialect::OpYamlInfoInterface,paddle::dialect::GetKernelTypeForVarInterface> {
 public:
  using Op::Op;
  static const char *name() { return "pd_op.moving_average_abs_max_scale"; }
  static const char *attributes_name[2];
  static constexpr uint32_t attributes_num = 2;
  static OpInfoTuple GetOpInfo();
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value in_accum_, pir::Value in_state_, float moving_rate=0.9f, bool is_test=false);
  
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value in_accum_, pir::Value in_state_, pir::AttributeMap attributes);
  
  void VerifySig();

  static phi::DataType GetKernelTypeForVar(
      const std::string& var_name,
        const phi::DataType& tensor_dtype,
        const phi::DataType& expected_kernel_dtype);




  static std::vector<std::vector<pir::Value>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);
  static void InferMeta(phi::InferMetaContext *infer_meta );
  static std::vector<pir::Type> InferMeta( const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes );
  pir::Value x() { return operand_source(0); }
  pir::Value in_accum() { return operand_source(1); }
  pir::Value in_state() { return operand_source(2); }
  pir::Value out() { return result(0); }
  pir::Value out_scale() { return result(1); }
  pir::Value out_state() { return result(2); }
  pir::Value out_accum() { return result(3); }

};

class  MovingAverageAbsMaxScale_Op : public pir::Op<MovingAverageAbsMaxScale_Op,paddle::dialect::InferMetaInterface,paddle::dialect::VjpInterface,paddle::dialect::OpYamlInfoInterface,paddle::dialect::GetKernelTypeForVarInterface,paddle::dialect::InplaceTrait> {
 public:
  using Op::Op;
  static const char *name() { return "pd_op.moving_average_abs_max_scale_"; }
  static const char *attributes_name[2];
  static constexpr uint32_t attributes_num = 2;
  static OpInfoTuple GetOpInfo();
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value in_accum_, pir::Value in_state_, float moving_rate=0.9f, bool is_test=false);
  
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value in_accum_, pir::Value in_state_, pir::AttributeMap attributes);
  
  void VerifySig();

  static phi::DataType GetKernelTypeForVar(
      const std::string& var_name,
        const phi::DataType& tensor_dtype,
        const phi::DataType& expected_kernel_dtype);




  static std::vector<std::vector<pir::Value>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);
  static void InferMeta(phi::InferMetaContext *infer_meta );
  static std::vector<pir::Type> InferMeta( const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes );
  pir::Value x() { return operand_source(0); }
  pir::Value in_accum() { return operand_source(1); }
  pir::Value in_state() { return operand_source(2); }
  pir::Value out() { return result(0); }
  pir::Value out_scale() { return result(1); }
  pir::Value out_state() { return result(2); }
  pir::Value out_accum() { return result(3); }

};


class  PartialSumGradOp : public pir::Op<PartialSumGradOp,paddle::dialect::InferMetaInterface,paddle::dialect::OpYamlInfoInterface,paddle::dialect::GetKernelTypeForVarInterface> {
 public:
  using Op::Op;
  static const char *name() { return "pd_op.partial_sum_grad"; }
  static const char *attributes_name[2];
  static constexpr uint32_t attributes_num = 2;
  static OpInfoTuple GetOpInfo();
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int start_index, int length);
  
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes);
  
  void VerifySig();

  static phi::DataType GetKernelTypeForVar(
      const std::string& var_name,
        const phi::DataType& tensor_dtype,
        const phi::DataType& expected_kernel_dtype);




  static std::vector<std::vector<pir::Value>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);
  static void InferMeta(phi::InferMetaContext *infer_meta );
  static std::vector<pir::Type> InferMeta( const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes );
  pir::Value x() { return operand_source(0); }
  pir::Value out_grad() { return operand_source(1); }
  pir::Value x_grad() { return result(0); }

};

class  StraightThroughEstimatorGradOp : public pir::Op<StraightThroughEstimatorGradOp,paddle::dialect::InferMetaInterface,paddle::dialect::OpYamlInfoInterface,paddle::dialect::GetKernelTypeForVarInterface> {
 public:
  using Op::Op;
  static const char *name() { return "pd_op.straight_through_estimator_grad"; }
  static const char *attributes_name[4];
  static constexpr uint32_t attributes_num = 4;
  static OpInfoTuple GetOpInfo();
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int moving_rate=0.9, int bit_length=8, int round_type=0, bool is_test=false);
  
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes);
  
  void VerifySig();

  static phi::DataType GetKernelTypeForVar(
      const std::string& var_name,
        const phi::DataType& tensor_dtype,
        const phi::DataType& expected_kernel_dtype);




  static std::vector<std::vector<pir::Value>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);
  static void InferMeta(phi::InferMetaContext *infer_meta );
  static std::vector<pir::Type> InferMeta( const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes );
  pir::Value x() { return operand_source(0); }
  pir::Value out_grad() { return operand_source(1); }
  pir::Value x_grad() { return result(0); }

};

class  StraightThroughEstimatorGrad2Op : public pir::Op<StraightThroughEstimatorGrad2Op,paddle::dialect::InferMetaInterface,paddle::dialect::OpYamlInfoInterface,paddle::dialect::GetKernelTypeForVarInterface> {
 public:
  using Op::Op;
  static const char *name() { return "pd_op.straight_through_estimator_grad2"; }
  static const char *attributes_name[3];
  static constexpr uint32_t attributes_num = 3;
  static OpInfoTuple GetOpInfo();
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, int bit_length=8, int round_type=0, bool is_test=false);
  
  static void Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value out_grad_, pir::AttributeMap attributes);
  
  void VerifySig();

  static phi::DataType GetKernelTypeForVar(
      const std::string& var_name,
        const phi::DataType& tensor_dtype,
        const phi::DataType& expected_kernel_dtype);




  static std::vector<std::vector<pir::Value>> Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients);
  static void InferMeta(phi::InferMetaContext *infer_meta );
  static std::vector<pir::Type> InferMeta( const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes );
  pir::Value x() { return operand_source(0); }
  pir::Value out_grad() { return operand_source(1); }
  pir::Value x_grad() { return result(0); }

};

} // namespace dialect
} // namespace paddle


IR_DECLARE_EXPLICIT_TYPE_ID(paddle::dialect::PartialSumOp)

IR_DECLARE_EXPLICIT_TYPE_ID(paddle::dialect::FakeQuantizeDequantizeMovingAverageAbsMaxOp)

IR_DECLARE_EXPLICIT_TYPE_ID(paddle::dialect::MovingAverageAbsMaxScaleOp)

IR_DECLARE_EXPLICIT_TYPE_ID(paddle::dialect::MovingAverageAbsMaxScale_Op)


IR_DECLARE_EXPLICIT_TYPE_ID(paddle::dialect::PartialSumGradOp)

IR_DECLARE_EXPLICIT_TYPE_ID(paddle::dialect::StraightThroughEstimatorGradOp)

IR_DECLARE_EXPLICIT_TYPE_ID(paddle::dialect::StraightThroughEstimatorGrad2Op)

