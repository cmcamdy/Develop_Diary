// This file is generated by "paddle/fluid/pir/dialect/op_generator/op_gen.py"
#include "/home/cmcandy/code/PD/Develop_Diary/code_reading/code_gen/test_codegen/pd_op.h"
#include "paddle/fluid/pir/dialect/kernel/ir/kernel_type.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_meta_tensor.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_selected_rows.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_tensor.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_attribute.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_type.h"
#include "paddle/fluid/primitive/rule/vjp/vjp.h"
#include "paddle/phi/api/lib/utils/allocator.h"
#include "paddle/phi/core/dense_tensor.h"
#include "paddle/phi/core/enforce.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/unary.h"
#include "paddle/pir/include/core/builtin_attribute.h"
#include "paddle/pir/include/core/builtin_op.h"
#include "paddle/pir/include/core/builtin_type.h"
#include "paddle/pir/include/core/ir_context.h"
#include "paddle/pir/include/core/op_base.h"

using namespace paddle::dialect;

namespace paddle {
namespace dialect {

void PartialSumOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PartialSumInferMeta);
  fn(infer_meta);
}


std::vector<pir::Type> PartialSumOp::InferMeta(const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes) {
  PADDLE_ENFORCE_NOT_NULL(
        p_attributes, common::errors::Fatal("AttrtibueMap pointer in InferMeta function is nullptr."));
  auto& attributes = *p_attributes; (void)attributes;

  PADDLE_ENFORCE_EQ(input_values.size() == 1, true, phi::errors::InvalidArgument(
      "Num of inputs is expected to be 1 but got %d.", input_values.size()));

  pir::Value x_ = input_values[0]; (void)x_;
  VLOG(4) << "Builder construction outputs";
  bool is_from_tensor = false; (void) is_from_tensor;
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;


  PADDLE_ENFORCE_NE(
      attributes.find("start_index"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'start_index' Attribute is expected for PartialSumOp. "));
  int start_index = attributes.at("start_index").dyn_cast<pir::Int32Attribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("length"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'length' Attribute is expected for PartialSumOp. "));
  int length = attributes.at("length").dyn_cast<pir::Int32Attribute>().data();

  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    if(x[i].isa<paddle::dialect::DenseTensorType>()) {
        auto x_type = x[i].dyn_cast<paddle::dialect::DenseTensorType>();
        vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x_type.dtype()),
                                                                    x_type.dims(),
                                                                    x_type.data_layout(),
                                                                    x_type.lod(),
                                                                    x_type.offset()));
    } else {
        PADDLE_THROW(phi::errors::Unimplemented("Only support DenseTensorType or AllocatedDenseTensorType"));
    }
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PartialSumInferMeta(meta_x, start_index, length, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_type = CvtToDenseTensorType(dense_out);

  argument_outputs.push_back(out_type);

  return argument_outputs;
}

const char *PartialSumOp::attributes_name[2] = { "start_index", "length" };

OpInfoTuple PartialSumOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("start_index", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("length", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PartialSumInferMeta", {"x", "start_index", "length"}, "partial_sum", {"x", "start_index", "length"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "partial_sum");
}

void PartialSumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int start_index, int length) {
  VLOG(4) << "Start build PartialSumOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_start_index = pir::Int32Attribute::get(pir::IrContext::Instance(), start_index);
  argument_attributes.insert({"start_index", attr_start_index});
  pir::Attribute attr_length = pir::Int32Attribute::get(pir::IrContext::Instance(), length);
  argument_attributes.insert({"length", attr_length});


  std::vector<pir::Type> argument_outputs = PartialSumOp::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void PartialSumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PartialSumOp";


  PADDLE_ENFORCE_NE(
      attributes.find("start_index"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'start_index' Attribute is expected for PartialSumOp. "));
  int start_index = attributes.at("start_index").dyn_cast<pir::Int32Attribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("length"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'length' Attribute is expected for PartialSumOp. "));
  int length = attributes.at("length").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_start_index = pir::Int32Attribute::get(pir::IrContext::Instance(), start_index);
  argument_attributes.insert({"start_index", attr_start_index});
  pir::Attribute attr_length = pir::Int32Attribute::get(pir::IrContext::Instance(), length);
  argument_attributes.insert({"length", attr_length});


  std::vector<pir::Type> argument_outputs = PartialSumOp::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void PartialSumOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PartialSumOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  PADDLE_ENFORCE_EQ(input_size == 1u, true, phi::errors::InvalidArgument(
                    "The size %d of inputs must be equal to 1.", input_size));
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        PADDLE_ENFORCE_EQ(vec_type[i].isa<paddle::dialect::DenseTensorType>(), true, phi::errors::InvalidArgument(
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type()));
      }
  }
  else {
    PADDLE_ENFORCE_EQ((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(), true, phi::errors::InvalidArgument(
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type()));
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  PADDLE_ENFORCE_GT(attributes.count("start_index"), 0, phi::errors::InvalidArgument(
                 "start_index does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("start_index").isa<pir::Int32Attribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: start_index is not pir::Int32Attribute."));

  PADDLE_ENFORCE_GT(attributes.count("length"), 0, phi::errors::InvalidArgument(
                 "length does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("length").isa<pir::Int32Attribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: length is not pir::Int32Attribute."));

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  PADDLE_ENFORCE_EQ(output_size == 1u, true, phi::errors::InvalidArgument(
                    "The size %d of outputs must be equal to 1.", output_size));
  PADDLE_ENFORCE_EQ((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(), true, phi::errors::InvalidArgument(
                 "Type validation failed for the 0th output."));
  }
  VLOG(4) << "End Verifying for: PartialSumOp.";
}

phi::DataType PartialSumOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PartialSumOp";
  


  return expected_kernel_dtype;
}

void FakeQuantizeDequantizeMovingAverageAbsMaxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FakeQuantizeMovingAverageInferMeta);
  fn(infer_meta);
}


std::vector<pir::Type> FakeQuantizeDequantizeMovingAverageAbsMaxOp::InferMeta(const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes) {
  PADDLE_ENFORCE_NOT_NULL(
        p_attributes, common::errors::Fatal("AttrtibueMap pointer in InferMeta function is nullptr."));
  auto& attributes = *p_attributes; (void)attributes;

  PADDLE_ENFORCE_EQ(input_values.size() == 4, true, phi::errors::InvalidArgument(
      "Num of inputs is expected to be 4 but got %d.", input_values.size()));

  pir::Value x_ = input_values[0]; (void)x_;
  VLOG(4) << "Builder construction outputs";
  bool is_from_tensor = false; (void) is_from_tensor;

  paddle::dialect::DenseTensorType x;
  if (x_.type().isa<paddle::dialect::DenseTensorType>()) {
    x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support paddle::dialect::DenseTensorType or paddle::dialect::AllocatedDenseTensorType"));
  }



  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_scale;
  paddle::dialect::IrMetaTensor meta_out_scale(&dense_out_scale);
  paddle::dialect::IrTensor dense_out_state;
  paddle::dialect::IrMetaTensor meta_out_state(&dense_out_state);
  paddle::dialect::IrTensor dense_out_accum;
  paddle::dialect::IrMetaTensor meta_out_accum(&dense_out_accum);

  phi::FakeQuantizeMovingAverageInferMeta(meta_x, &meta_out, &meta_out_scale, &meta_out_state, &meta_out_accum);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_type = CvtToDenseTensorType(dense_out);

  pir::Type out_scale_type = CvtToDenseTensorType(dense_out_scale);

  pir::Type out_state_type = CvtToDenseTensorType(dense_out_state);

  pir::Type out_accum_type = CvtToDenseTensorType(dense_out_accum);

  argument_outputs.push_back(out_type);

  argument_outputs.push_back(out_scale_type);

  argument_outputs.push_back(out_state_type);

  argument_outputs.push_back(out_accum_type);

  return argument_outputs;
}

const char *FakeQuantizeDequantizeMovingAverageAbsMaxOp::attributes_name[4] = { "moving_rate", "bit_length", "round_type", "is_test" };

OpInfoTuple FakeQuantizeDequantizeMovingAverageAbsMaxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("in_scale", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("in_accum", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("in_state", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("moving_rate", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("bit_length", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("round_type", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_scale", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_state", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("out_accum", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FakeQuantizeMovingAverageInferMeta", {"x"}, "fake_quantize_dequantize_moving_average_abs_max", {"x", "in_scale", "in_accum", "in_state", "moving_rate", "bit_length", "round_type", "is_test"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fake_quantize_dequantize_moving_average_abs_max");
}

void FakeQuantizeDequantizeMovingAverageAbsMaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value in_scale_, pir::Value in_accum_, pir::Value in_state_, float moving_rate, int bit_length, int round_type, bool is_test) {
  VLOG(4) << "Start build FakeQuantizeDequantizeMovingAverageAbsMaxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, in_scale_, in_accum_, in_state_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_moving_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), moving_rate);
  argument_attributes.insert({"moving_rate", attr_moving_rate});
  pir::Attribute attr_bit_length = pir::Int32Attribute::get(pir::IrContext::Instance(), bit_length);
  argument_attributes.insert({"bit_length", attr_bit_length});
  pir::Attribute attr_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), round_type);
  argument_attributes.insert({"round_type", attr_round_type});
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument_attributes.insert({"is_test", attr_is_test});


  std::vector<pir::Type> argument_outputs = FakeQuantizeDequantizeMovingAverageAbsMaxOp::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void FakeQuantizeDequantizeMovingAverageAbsMaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value in_scale_, pir::Value in_accum_, pir::Value in_state_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FakeQuantizeDequantizeMovingAverageAbsMaxOp";


  PADDLE_ENFORCE_NE(
      attributes.find("moving_rate"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'moving_rate' Attribute is expected for FakeQuantizeDequantizeMovingAverageAbsMaxOp. "));
  float moving_rate = attributes.at("moving_rate").dyn_cast<pir::FloatAttribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("bit_length"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'bit_length' Attribute is expected for FakeQuantizeDequantizeMovingAverageAbsMaxOp. "));
  int bit_length = attributes.at("bit_length").dyn_cast<pir::Int32Attribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("round_type"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'round_type' Attribute is expected for FakeQuantizeDequantizeMovingAverageAbsMaxOp. "));
  int round_type = attributes.at("round_type").dyn_cast<pir::Int32Attribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("is_test"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'is_test' Attribute is expected for FakeQuantizeDequantizeMovingAverageAbsMaxOp. "));
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, in_scale_, in_accum_, in_state_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_moving_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), moving_rate);
  argument_attributes.insert({"moving_rate", attr_moving_rate});
  pir::Attribute attr_bit_length = pir::Int32Attribute::get(pir::IrContext::Instance(), bit_length);
  argument_attributes.insert({"bit_length", attr_bit_length});
  pir::Attribute attr_round_type = pir::Int32Attribute::get(pir::IrContext::Instance(), round_type);
  argument_attributes.insert({"round_type", attr_round_type});
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument_attributes.insert({"is_test", attr_is_test});


  std::vector<pir::Type> argument_outputs = FakeQuantizeDequantizeMovingAverageAbsMaxOp::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void FakeQuantizeDequantizeMovingAverageAbsMaxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FakeQuantizeDequantizeMovingAverageAbsMaxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  PADDLE_ENFORCE_EQ(input_size == 4u, true, phi::errors::InvalidArgument(
                    "The size %d of inputs must be equal to 4.", input_size));
  PADDLE_ENFORCE_EQ((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(), true,
                  phi::errors::InvalidArgument("Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type()));
  PADDLE_ENFORCE_EQ((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(), true,
                  phi::errors::InvalidArgument("Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type()));
  PADDLE_ENFORCE_EQ((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(), true,
                  phi::errors::InvalidArgument("Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type()));
  PADDLE_ENFORCE_EQ((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(), true,
                  phi::errors::InvalidArgument("Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type()));
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  PADDLE_ENFORCE_GT(attributes.count("moving_rate"), 0, phi::errors::InvalidArgument(
                 "moving_rate does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("moving_rate").isa<pir::FloatAttribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: moving_rate is not pir::FloatAttribute."));

  PADDLE_ENFORCE_GT(attributes.count("bit_length"), 0, phi::errors::InvalidArgument(
                 "bit_length does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("bit_length").isa<pir::Int32Attribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: bit_length is not pir::Int32Attribute."));

  PADDLE_ENFORCE_GT(attributes.count("round_type"), 0, phi::errors::InvalidArgument(
                 "round_type does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("round_type").isa<pir::Int32Attribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: round_type is not pir::Int32Attribute."));

  PADDLE_ENFORCE_GT(attributes.count("is_test"), 0, phi::errors::InvalidArgument(
                 "is_test does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("is_test").isa<pir::BoolAttribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: is_test is not pir::BoolAttribute."));

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  PADDLE_ENFORCE_EQ(output_size == 4u, true, phi::errors::InvalidArgument(
                    "The size %d of outputs must be equal to 4.", output_size));
  PADDLE_ENFORCE_EQ((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(), true, phi::errors::InvalidArgument(
                 "Type validation failed for the 0th output."));
  PADDLE_ENFORCE_EQ((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(), true, phi::errors::InvalidArgument(
                 "Type validation failed for the 1th output."));
  if (auto output_2_type = (*this)->result(2).type()) {
    PADDLE_ENFORCE_EQ(output_2_type.isa<paddle::dialect::DenseTensorType>(),true, phi::errors::InvalidArgument(
                   "Type validation failed for the 2th output."));
  }
  if (auto output_3_type = (*this)->result(3).type()) {
    PADDLE_ENFORCE_EQ(output_3_type.isa<paddle::dialect::DenseTensorType>(),true, phi::errors::InvalidArgument(
                   "Type validation failed for the 3th output."));
  }
  }
  VLOG(4) << "End Verifying for: FakeQuantizeDequantizeMovingAverageAbsMaxOp.";
}

phi::DataType FakeQuantizeDequantizeMovingAverageAbsMaxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FakeQuantizeDequantizeMovingAverageAbsMaxOp";
  


  return expected_kernel_dtype;
}

void MovingAverageAbsMaxScaleOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MovingAverageAbsMaxScaleInferMeta);
  fn(infer_meta);
}


std::vector<pir::Type> MovingAverageAbsMaxScaleOp::InferMeta(const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes) {
  PADDLE_ENFORCE_NOT_NULL(
        p_attributes, common::errors::Fatal("AttrtibueMap pointer in InferMeta function is nullptr."));
  auto& attributes = *p_attributes; (void)attributes;

  PADDLE_ENFORCE_EQ(input_values.size() == 3, true, phi::errors::InvalidArgument(
      "Num of inputs is expected to be 3 but got %d.", input_values.size()));

  pir::Value x_ = input_values[0]; (void)x_;
  pir::Value in_accum_ = input_values[1]; (void)in_accum_;
  pir::Value in_state_ = input_values[2]; (void)in_state_;
  VLOG(4) << "Builder construction outputs";
  bool is_from_tensor = false; (void) is_from_tensor;

  paddle::dialect::DenseTensorType x;
  if (x_.type().isa<paddle::dialect::DenseTensorType>()) {
    x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support paddle::dialect::DenseTensorType or paddle::dialect::AllocatedDenseTensorType"));
  }



  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_in_accum;
  paddle::dialect::IrTensor ir_tensor_in_accum;

  if (in_accum_.impl() != nullptr) {
    VLOG(4) << "Builder construction  dense_in_accum";
    paddle::dialect::DenseTensorType in_accum;
    if (in_accum_.type().isa<paddle::dialect::DenseTensorType>()) {
      in_accum = in_accum_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    } else {
      PADDLE_THROW(phi::errors::Unimplemented("Only support paddle::dialect::DenseTensorType or paddle::dialect::AllocatedDenseTensorType"));
    }
    ir_tensor_in_accum = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(in_accum.dtype()),
                                                        in_accum.dims(),
                                                        in_accum.data_layout(),
                                                        in_accum.lod(),
                                                        in_accum.offset());
    VLOG(4) << "Builder construction  meta_in_accum";
    meta_in_accum = paddle::dialect::IrMetaTensor(&ir_tensor_in_accum);
  }


  paddle::dialect::IrMetaTensor meta_in_state;
  paddle::dialect::IrTensor ir_tensor_in_state;

  if (in_state_.impl() != nullptr) {
    VLOG(4) << "Builder construction  dense_in_state";
    paddle::dialect::DenseTensorType in_state;
    if (in_state_.type().isa<paddle::dialect::DenseTensorType>()) {
      in_state = in_state_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    } else {
      PADDLE_THROW(phi::errors::Unimplemented("Only support paddle::dialect::DenseTensorType or paddle::dialect::AllocatedDenseTensorType"));
    }
    ir_tensor_in_state = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(in_state.dtype()),
                                                        in_state.dims(),
                                                        in_state.data_layout(),
                                                        in_state.lod(),
                                                        in_state.offset());
    VLOG(4) << "Builder construction  meta_in_state";
    meta_in_state = paddle::dialect::IrMetaTensor(&ir_tensor_in_state);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_scale;
  paddle::dialect::IrMetaTensor meta_out_scale(&dense_out_scale);
  paddle::dialect::IrTensor dense_out_state;
  paddle::dialect::IrMetaTensor meta_out_state(&dense_out_state);
  paddle::dialect::IrTensor dense_out_accum;
  paddle::dialect::IrMetaTensor meta_out_accum(&dense_out_accum);

  phi::MovingAverageAbsMaxScaleInferMeta(meta_x, meta_in_accum, meta_in_state, &meta_out, &meta_out_scale, &meta_out_state, &meta_out_accum);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_type = CvtToDenseTensorType(dense_out);

  pir::Type out_scale_type = CvtToDenseTensorType(dense_out_scale);

  pir::Type out_state_type;
  if (in_state_.impl() != nullptr) {
    out_state_type = CvtToDenseTensorType(dense_out_state);
  }

  pir::Type out_accum_type;
  if (in_accum_.impl() != nullptr) {
    out_accum_type = CvtToDenseTensorType(dense_out_accum);
  }

  argument_outputs.push_back(out_type);

  argument_outputs.push_back(out_scale_type);

  argument_outputs.push_back(out_state_type);

  argument_outputs.push_back(out_accum_type);

  return argument_outputs;
}

const char *MovingAverageAbsMaxScaleOp::attributes_name[2] = { "moving_rate", "is_test" };

OpInfoTuple MovingAverageAbsMaxScaleOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("in_accum", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("in_state", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("moving_rate", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("out_scale", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_state", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("out_accum", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MovingAverageAbsMaxScaleInferMeta", {"x", "in_accum", "in_state"}, "moving_average_abs_max_scale", {"x", "in_accum", "in_state", "moving_rate", "is_test"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "moving_average_abs_max_scale");
}

void MovingAverageAbsMaxScaleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value in_accum_, pir::Value in_state_, float moving_rate, bool is_test) {
  VLOG(4) << "Start build MovingAverageAbsMaxScaleOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, in_accum_, in_state_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_moving_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), moving_rate);
  argument_attributes.insert({"moving_rate", attr_moving_rate});
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument_attributes.insert({"is_test", attr_is_test});


  std::vector<pir::Type> argument_outputs = MovingAverageAbsMaxScaleOp::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void MovingAverageAbsMaxScaleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value in_accum_, pir::Value in_state_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MovingAverageAbsMaxScaleOp";


  PADDLE_ENFORCE_NE(
      attributes.find("moving_rate"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'moving_rate' Attribute is expected for MovingAverageAbsMaxScaleOp. "));
  float moving_rate = attributes.at("moving_rate").dyn_cast<pir::FloatAttribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("is_test"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'is_test' Attribute is expected for MovingAverageAbsMaxScaleOp. "));
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, in_accum_, in_state_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_moving_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), moving_rate);
  argument_attributes.insert({"moving_rate", attr_moving_rate});
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument_attributes.insert({"is_test", attr_is_test});


  std::vector<pir::Type> argument_outputs = MovingAverageAbsMaxScaleOp::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void MovingAverageAbsMaxScaleOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MovingAverageAbsMaxScaleOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  PADDLE_ENFORCE_EQ(input_size == 3u, true, phi::errors::InvalidArgument(
                    "The size %d of inputs must be equal to 3.", input_size));
  PADDLE_ENFORCE_EQ((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(), true,
                  phi::errors::InvalidArgument("Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type()));
  if (auto val = (*this)->operand(1)) {
    PADDLE_ENFORCE_EQ(val.type().isa<paddle::dialect::DenseTensorType>(), true, phi::errors::InvalidArgument(
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type()));
  }
  if (auto val = (*this)->operand(2)) {
    PADDLE_ENFORCE_EQ(val.type().isa<paddle::dialect::DenseTensorType>(), true, phi::errors::InvalidArgument(
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type()));
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  PADDLE_ENFORCE_GT(attributes.count("moving_rate"), 0, phi::errors::InvalidArgument(
                 "moving_rate does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("moving_rate").isa<pir::FloatAttribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: moving_rate is not pir::FloatAttribute."));

  PADDLE_ENFORCE_GT(attributes.count("is_test"), 0, phi::errors::InvalidArgument(
                 "is_test does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("is_test").isa<pir::BoolAttribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: is_test is not pir::BoolAttribute."));

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  PADDLE_ENFORCE_EQ(output_size == 4u, true, phi::errors::InvalidArgument(
                    "The size %d of outputs must be equal to 4.", output_size));
  if (auto output_0_type = (*this)->result(0).type()) {
    PADDLE_ENFORCE_EQ(output_0_type.isa<paddle::dialect::DenseTensorType>(),true, phi::errors::InvalidArgument(
                   "Type validation failed for the 0th output."));
  }
  PADDLE_ENFORCE_EQ((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(), true, phi::errors::InvalidArgument(
                 "Type validation failed for the 1th output."));
  if (auto output_2_type = (*this)->result(2).type()) {
    PADDLE_ENFORCE_EQ(output_2_type.isa<paddle::dialect::DenseTensorType>(),true, phi::errors::InvalidArgument(
                   "Type validation failed for the 2th output."));
  }
  if (auto output_3_type = (*this)->result(3).type()) {
    PADDLE_ENFORCE_EQ(output_3_type.isa<paddle::dialect::DenseTensorType>(),true, phi::errors::InvalidArgument(
                   "Type validation failed for the 3th output."));
  }
  }
  VLOG(4) << "End Verifying for: MovingAverageAbsMaxScaleOp.";
}

phi::DataType MovingAverageAbsMaxScaleOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MovingAverageAbsMaxScaleOp";
  


  return expected_kernel_dtype;
}

void MovingAverageAbsMaxScale_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MovingAverageAbsMaxScaleInferMeta);
  fn(infer_meta);
}


std::vector<pir::Type> MovingAverageAbsMaxScale_Op::InferMeta(const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes) {
  PADDLE_ENFORCE_NOT_NULL(
        p_attributes, common::errors::Fatal("AttrtibueMap pointer in InferMeta function is nullptr."));
  auto& attributes = *p_attributes; (void)attributes;

  PADDLE_ENFORCE_EQ(input_values.size() == 3, true, phi::errors::InvalidArgument(
      "Num of inputs is expected to be 3 but got %d.", input_values.size()));

  pir::Value x_ = input_values[0]; (void)x_;
  pir::Value in_accum_ = input_values[1]; (void)in_accum_;
  pir::Value in_state_ = input_values[2]; (void)in_state_;
  VLOG(4) << "Builder construction outputs";
  bool is_from_tensor = false; (void) is_from_tensor;

  paddle::dialect::DenseTensorType x;
  if (x_.type().isa<paddle::dialect::DenseTensorType>()) {
    x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support paddle::dialect::DenseTensorType or paddle::dialect::AllocatedDenseTensorType"));
  }



  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_in_accum;
  paddle::dialect::IrTensor ir_tensor_in_accum;

  if (in_accum_.impl() != nullptr) {
    VLOG(4) << "Builder construction  dense_in_accum";
    paddle::dialect::DenseTensorType in_accum;
    if (in_accum_.type().isa<paddle::dialect::DenseTensorType>()) {
      in_accum = in_accum_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    } else {
      PADDLE_THROW(phi::errors::Unimplemented("Only support paddle::dialect::DenseTensorType or paddle::dialect::AllocatedDenseTensorType"));
    }
    ir_tensor_in_accum = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(in_accum.dtype()),
                                                        in_accum.dims(),
                                                        in_accum.data_layout(),
                                                        in_accum.lod(),
                                                        in_accum.offset());
    VLOG(4) << "Builder construction  meta_in_accum";
    meta_in_accum = paddle::dialect::IrMetaTensor(&ir_tensor_in_accum);
  }


  paddle::dialect::IrMetaTensor meta_in_state;
  paddle::dialect::IrTensor ir_tensor_in_state;

  if (in_state_.impl() != nullptr) {
    VLOG(4) << "Builder construction  dense_in_state";
    paddle::dialect::DenseTensorType in_state;
    if (in_state_.type().isa<paddle::dialect::DenseTensorType>()) {
      in_state = in_state_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    } else {
      PADDLE_THROW(phi::errors::Unimplemented("Only support paddle::dialect::DenseTensorType or paddle::dialect::AllocatedDenseTensorType"));
    }
    ir_tensor_in_state = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(in_state.dtype()),
                                                        in_state.dims(),
                                                        in_state.data_layout(),
                                                        in_state.lod(),
                                                        in_state.offset());
    VLOG(4) << "Builder construction  meta_in_state";
    meta_in_state = paddle::dialect::IrMetaTensor(&ir_tensor_in_state);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_out_scale;
  paddle::dialect::IrMetaTensor meta_out_scale(&dense_out_scale);
  paddle::dialect::IrTensor dense_out_state;
  paddle::dialect::IrMetaTensor meta_out_state(&dense_out_state);
  paddle::dialect::IrTensor dense_out_accum;
  paddle::dialect::IrMetaTensor meta_out_accum(&dense_out_accum);

  phi::MovingAverageAbsMaxScaleInferMeta(meta_x, meta_in_accum, meta_in_state, &meta_out, &meta_out_scale, &meta_out_state, &meta_out_accum);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_type = CvtToDenseTensorType(dense_out);

  pir::Type out_scale_type = CvtToDenseTensorType(dense_out_scale);

  pir::Type out_state_type;
  if (in_state_.impl() != nullptr) {
    out_state_type = CvtToDenseTensorType(dense_out_state);
  }

  pir::Type out_accum_type;
  if (in_accum_.impl() != nullptr) {
    out_accum_type = CvtToDenseTensorType(dense_out_accum);
  }

  argument_outputs.push_back(out_type);

  argument_outputs.push_back(out_scale_type);

  argument_outputs.push_back(out_state_type);

  argument_outputs.push_back(out_accum_type);

  return argument_outputs;
}

const char *MovingAverageAbsMaxScale_Op::attributes_name[2] = { "moving_rate", "is_test" };

OpInfoTuple MovingAverageAbsMaxScale_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("in_accum", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("in_state", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("moving_rate", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("out_scale", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_state", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("out_accum", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MovingAverageAbsMaxScaleInferMeta", {"x", "in_accum", "in_state"}, "moving_average_abs_max_scale", {"x", "in_accum", "in_state", "moving_rate", "is_test"}, {}, {}, {{"out_accum", "in_accum"},{"out_state", "in_state"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "moving_average_abs_max_scale");
}

void MovingAverageAbsMaxScale_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value in_accum_, pir::Value in_state_, float moving_rate, bool is_test) {
  VLOG(4) << "Start build MovingAverageAbsMaxScale_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, in_accum_, in_state_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_moving_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), moving_rate);
  argument_attributes.insert({"moving_rate", attr_moving_rate});
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument_attributes.insert({"is_test", attr_is_test});


  std::vector<pir::Type> argument_outputs = MovingAverageAbsMaxScale_Op::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void MovingAverageAbsMaxScale_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value in_accum_, pir::Value in_state_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MovingAverageAbsMaxScale_Op";


  PADDLE_ENFORCE_NE(
      attributes.find("moving_rate"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'moving_rate' Attribute is expected for MovingAverageAbsMaxScale_Op. "));
  float moving_rate = attributes.at("moving_rate").dyn_cast<pir::FloatAttribute>().data();

  PADDLE_ENFORCE_NE(
      attributes.find("is_test"),
      attributes.end(),
      phi::errors::InvalidArgument(
          "'is_test' Attribute is expected for MovingAverageAbsMaxScale_Op. "));
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, in_accum_, in_state_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};
  pir::Attribute attr_moving_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), moving_rate);
  argument_attributes.insert({"moving_rate", attr_moving_rate});
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument_attributes.insert({"is_test", attr_is_test});


  std::vector<pir::Type> argument_outputs = MovingAverageAbsMaxScale_Op::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void MovingAverageAbsMaxScale_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MovingAverageAbsMaxScale_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  PADDLE_ENFORCE_EQ(input_size == 3u, true, phi::errors::InvalidArgument(
                    "The size %d of inputs must be equal to 3.", input_size));
  PADDLE_ENFORCE_EQ((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(), true,
                  phi::errors::InvalidArgument("Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type()));
  if (auto val = (*this)->operand(1)) {
    PADDLE_ENFORCE_EQ(val.type().isa<paddle::dialect::DenseTensorType>(), true, phi::errors::InvalidArgument(
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type()));
  }
  if (auto val = (*this)->operand(2)) {
    PADDLE_ENFORCE_EQ(val.type().isa<paddle::dialect::DenseTensorType>(), true, phi::errors::InvalidArgument(
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type()));
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  PADDLE_ENFORCE_GT(attributes.count("moving_rate"), 0, phi::errors::InvalidArgument(
                 "moving_rate does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("moving_rate").isa<pir::FloatAttribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: moving_rate is not pir::FloatAttribute."));

  PADDLE_ENFORCE_GT(attributes.count("is_test"), 0, phi::errors::InvalidArgument(
                 "is_test does not exist."));
  PADDLE_ENFORCE_EQ(attributes.at("is_test").isa<pir::BoolAttribute>(), true, phi::errors::InvalidArgument(
                 "Type of attribute: is_test is not pir::BoolAttribute."));

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  PADDLE_ENFORCE_EQ(output_size == 4u, true, phi::errors::InvalidArgument(
                    "The size %d of outputs must be equal to 4.", output_size));
  if (auto output_0_type = (*this)->result(0).type()) {
    PADDLE_ENFORCE_EQ(output_0_type.isa<paddle::dialect::DenseTensorType>(),true, phi::errors::InvalidArgument(
                   "Type validation failed for the 0th output."));
  }
  PADDLE_ENFORCE_EQ((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(), true, phi::errors::InvalidArgument(
                 "Type validation failed for the 1th output."));
  if (auto output_2_type = (*this)->result(2).type()) {
    PADDLE_ENFORCE_EQ(output_2_type.isa<paddle::dialect::DenseTensorType>(),true, phi::errors::InvalidArgument(
                   "Type validation failed for the 2th output."));
  }
  if (auto output_3_type = (*this)->result(3).type()) {
    PADDLE_ENFORCE_EQ(output_3_type.isa<paddle::dialect::DenseTensorType>(),true, phi::errors::InvalidArgument(
                   "Type validation failed for the 3th output."));
  }
  }
  VLOG(4) << "End Verifying for: MovingAverageAbsMaxScale_Op.";
}

phi::DataType MovingAverageAbsMaxScale_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MovingAverageAbsMaxScale_Op";
  


  return expected_kernel_dtype;
}

void StraightThroughEstimatorGradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}


std::vector<pir::Type> StraightThroughEstimatorGradOp::InferMeta(const std::vector<pir::Value>& input_values, pir::AttributeMap* p_attributes) {
  PADDLE_ENFORCE_NOT_NULL(
        p_attributes, common::errors::Fatal("AttrtibueMap pointer in InferMeta function is nullptr."));
  auto& attributes = *p_attributes; (void)attributes;

  PADDLE_ENFORCE_EQ(input_values.size() == 1, true, phi::errors::InvalidArgument(
      "Num of inputs is expected to be 1 but got %d.", input_values.size()));

  pir::Value out_grad_ = input_values[0]; (void)out_grad_;
  VLOG(4) << "Builder construction outputs";
  bool is_from_tensor = false; (void) is_from_tensor;

  paddle::dialect::DenseTensorType out_grad;
  if (out_grad_.type().isa<paddle::dialect::DenseTensorType>()) {
    out_grad = out_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_grad;
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support paddle::dialect::DenseTensorType or paddle::dialect::AllocatedDenseTensorType"));
  }



  VLOG(4) << "Builder construction  dense_out_grad";
  paddle::dialect::IrTensor ir_tensor_out_grad(paddle::dialect::TransToPhiDataType(out_grad.dtype()),
                                                      out_grad.dims(),
                                                      out_grad.data_layout(),
                                                      out_grad.lod(),
                                                      out_grad.offset());
  VLOG(4) << "Builder construction  meta_out_grad";
  paddle::dialect::IrMetaTensor meta_out_grad(&ir_tensor_out_grad);
  paddle::dialect::IrTensor dense_x_grad;
  paddle::dialect::IrMetaTensor meta_x_grad(&dense_x_grad);

  phi::UnchangedInferMeta(meta_out_grad, &meta_x_grad);

  std::vector<pir::Type> argument_outputs;
  pir::Type x_grad_type = CvtToDenseTensorType(dense_x_grad);

  argument_outputs.push_back(x_grad_type);

  return argument_outputs;
}

OpInfoTuple StraightThroughEstimatorGradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("out_grad", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("x_grad", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"out_grad"}, "straight_through_estimator_grad", {"out_grad"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "straight_through_estimator_grad");
}

void StraightThroughEstimatorGradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value out_grad_) {
  VLOG(4) << "Start build StraightThroughEstimatorGradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {out_grad_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::AttributeMap argument_attributes = {};


  std::vector<pir::Type> argument_outputs = StraightThroughEstimatorGradOp::InferMeta(argument_inputs, &argument_attributes);
  argument.AddAttributes(argument_attributes);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);
}

void StraightThroughEstimatorGradOp::VerifySig() {}

phi::DataType StraightThroughEstimatorGradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: StraightThroughEstimatorGradOp";
  


  return expected_kernel_dtype;
}

} // namespace dialect
} // namespace paddle


IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PartialSumOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FakeQuantizeDequantizeMovingAverageAbsMaxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MovingAverageAbsMaxScaleOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MovingAverageAbsMaxScale_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::StraightThroughEstimatorGradOp)

