// This file is generated by "paddle/fluid/pir/dialect/op_generator/op_gen.py"
#include "paddle/fluid/pir/dialect/operator/ir/op_attribute.h"
#include "paddle/fluid/pir/dialect/operator/ir/pd_op.h"
#include "paddle/fluid/pir/dialect/operator/utils/utils.h"
#include "paddle/fluid/primitive/rule/vjp/vjp.h"
#include "paddle/fluid/primitive/type/lazy_tensor.h"
#include "paddle/phi/common/int_array.h"
#include "paddle/pir/include/core/builtin_op.h"
#include "paddle/pir/include/core/op_base.h"

namespace paddle {
namespace dialect {

std::vector<std::vector<pir::Value>> PartialSumOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      1,
      platform::errors::InvalidArgument("partial_sum op's inputs size should be 1, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      1,
      platform::errors::InvalidArgument("partial_sum op's outputs size should be 1, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of partial_sum_grad";

    std::vector<Tensor> x;
    for (size_t idx = 0; idx < inputs_[0].size(); idx++) {
        x.emplace_back(
            std::make_shared<primitive::LazyTensor>(inputs_[0][idx]));
    }
    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of partial_sum_grad";

    int start_index = op->attribute("start_index").dyn_cast<pir::Int32Attribute>().data();
    int length = op->attribute("length").dyn_cast<pir::Int32Attribute>().data();

    VLOG(6) << "Vjp prepare call partial_sum's vjp interface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::partial_sum_vjp(
        x, out_grad, start_index, length, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of partial_sum_grad";

    std::vector<std::vector<pir::Value>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::Value>> FakeQuantizeDequantizeMovingAverageAbsMaxOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      4,
      platform::errors::InvalidArgument("fake_quantize_dequantize_moving_average_abs_max op's inputs size should be 4, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      4,
      platform::errors::InvalidArgument("fake_quantize_dequantize_moving_average_abs_max op's outputs size should be 4, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of fake_quantize_dequantize_moving_average_abs_max_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of fake_quantize_dequantize_moving_average_abs_max_grad";

    int moving_rate = op->attribute("moving_rate").dyn_cast<pir::Int32Attribute>().data();
    int bit_length = op->attribute("bit_length").dyn_cast<pir::Int32Attribute>().data();
    int round_type = op->attribute("round_type").dyn_cast<pir::Int32Attribute>().data();
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call fake_quantize_dequantize_moving_average_abs_max's vjp interface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::fake_quantize_dequantize_moving_average_abs_max_vjp(
        out_grad, moving_rate, bit_length, round_type, is_test, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of fake_quantize_dequantize_moving_average_abs_max_grad";

    std::vector<std::vector<pir::Value>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::Value>> MovingAverageAbsMaxScaleOp::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("moving_average_abs_max_scale op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      4,
      platform::errors::InvalidArgument("moving_average_abs_max_scale op's outputs size should be 4, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of moving_average_abs_max_scale_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of moving_average_abs_max_scale_grad";

    int bit_length = 8;
    int round_type = 0;
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call moving_average_abs_max_scale's vjp interface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::moving_average_abs_max_scale_vjp(
        out_grad, bit_length, round_type, is_test, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of moving_average_abs_max_scale_grad";

    std::vector<std::vector<pir::Value>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value();
            }
        }
    }
    return res;
}

std::vector<std::vector<pir::Value>> MovingAverageAbsMaxScale_Op::Vjp(pir::Operation* op, const std::vector<std::vector<pir::Value>>& inputs_, const std::vector<std::vector<pir::Value>>& outputs, const std::vector<std::vector<pir::Value>>& out_grads, const std::vector<std::vector<bool>>& stop_gradients){

    PADDLE_ENFORCE_EQ(
      inputs_.size(),
      3,
      platform::errors::InvalidArgument("moving_average_abs_max_scale op's inputs size should be 3, but now is %d.", inputs_.size()));
    PADDLE_ENFORCE_EQ(
      outputs.size(),
      4,
      platform::errors::InvalidArgument("moving_average_abs_max_scale op's outputs size should be 4, but now is %d.", outputs.size()));

    VLOG(6) << "Prepare inputs of moving_average_abs_max_scale_grad";

    Tensor out_grad(std::make_shared<primitive::LazyTensor>(out_grads[0][0]));

    VLOG(6) << "Vjp prepare Prepare attributes of moving_average_abs_max_scale_grad";

    int bit_length = 8;
    int round_type = 0;
    bool is_test = op->attribute("is_test").dyn_cast<pir::BoolAttribute>().data();

    VLOG(6) << "Vjp prepare call moving_average_abs_max_scale_'s vjp interface";

    std::vector<std::vector<Tensor>> tensor_res =
        primitive::moving_average_abs_max_scale_vjp(
        out_grad, bit_length, round_type, is_test, stop_gradients);

    VLOG(6) << "Vjp prepare stop gradient of moving_average_abs_max_scale_grad";

    std::vector<std::vector<pir::Value>> res(tensor_res.size());
    for (size_t i = 0; i < tensor_res.size(); ++i) {
        res[i].resize(tensor_res[i].size());
        for (size_t j = 0; j < tensor_res[i].size(); ++j) {
            if(tensor_res[i][j].defined()){
                res[i][j] = std::static_pointer_cast<primitive::LazyTensor>(tensor_res[i][j].impl())->value();
            }
        }
    }
    return res;
}


}  // namespace dialect
}  // namespace paddle
